{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp7dn01wwN3k"
      },
      "source": [
        "# PII Anonymization Tool\n",
        "\n",
        "This notebook demonstrates a named entity recognition (NER) system for detecting and anonymizing personally identifiable information (PII) in text.\n",
        "\n",
        "Till the end:\n",
        "1. Train a model to recognize named entities (if needed)\n",
        "2. Use the model to anonymize text by replacing entities\n",
        "3. Try different anonymization styles\n",
        "4. Interactively use the system through a web interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeGiHRkFwN3m"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_model(output_dir=\"./model\", force_train=False):\n",
        "    #model existance check, will not train if already exists.\n",
        "    exists, is_valid = check_model_exists(output_dir)\n",
        "\n",
        "    if exists and is_valid and not force_train:\n",
        "        print(f\"âœ“ Valid trained model already exists at {output_dir}\")\n",
        "        print(\"  Skipping training to save time.\")\n",
        "    else:\n",
        "        #data loading and preparing\n",
        "        print(\"\\n=== Data Loading and Preparation ===\")\n",
        "        processor = PIIDataProcessor()\n",
        "        datasets = processor.load_conll2003_dataset()\n",
        "\n",
        "        # Debugging:\n",
        "        print(\"Available columns in the dataset:\")\n",
        "        print(datasets[\"train\"].column_names)\n",
        "\n",
        "        datasets = processor.convert_to_ner_format(datasets)\n"
      ],
      "metadata": {
        "id": "F3SQfKNK67R5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RyJOVlRxwN3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce39d39b-1176-4ac6-ca06-fcbb867ceb23"
      },
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets seqeval flask gradio"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kTXu8EvwN3o"
      },
      "source": [
        "## 2. Data Processor\n",
        "\n",
        "Next, let's create the data processor class that will handle loading and processing the CoNLL-2003 dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/panchalaman/PII-Anonymization-System/raw/refs/heads/main/AI%20Applications%20for%20Digital%20Business%20-%20B198c7/conll2003.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05OP4lOfdn6E",
        "outputId": "99e3001d-3081-4b7c-c0ed-a92b97614e76"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 17:18:55--  https://github.com/panchalaman/PII-Anonymization-System/raw/refs/heads/main/AI%20Applications%20for%20Digital%20Business%20-%20B198c7/conll2003.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/panchalaman/PII-Anonymization-System/refs/heads/main/AI%20Applications%20for%20Digital%20Business%20-%20B198c7/conll2003.zip [following]\n",
            "--2025-03-26 17:18:55--  https://raw.githubusercontent.com/panchalaman/PII-Anonymization-System/refs/heads/main/AI%20Applications%20for%20Digital%20Business%20-%20B198c7/conll2003.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 982975 (960K) [application/zip]\n",
            "Saving to: â€˜conll2003.zipâ€™\n",
            "\n",
            "conll2003.zip       100%[===================>] 959.94K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-03-26 17:18:56 (27.0 MB/s) - â€˜conll2003.zipâ€™ saved [982975/982975]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "opTtUagNwN3o"
      },
      "source": [
        "from typing import Dict, List\n",
        "from datasets import load_dataset\n",
        "\n",
        "class PIIDataProcessor:\n",
        "    \"\"\"\n",
        "    Processes data for named entity recognition.\n",
        "\n",
        "    This class loads the CoNLL-2003 dataset and prepares it for training\n",
        "    a named entity recognition model focused on PII detection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the data processor with CoNLL-2003 entity labels.\n",
        "\n",
        "        Entity types are:\n",
        "        - PER: Person\n",
        "        - ORG: Organization\n",
        "        - LOC: Location\n",
        "        - MISC: Miscellaneous\n",
        "        \"\"\"\n",
        "        # defiinnig NER labels from CoNLL-2003\n",
        "        self.label_list = [\n",
        "            \"O\",  # Not an entity\n",
        "            \"B-PER\", \"I-PER\",  # Person entities\n",
        "            \"B-ORG\", \"I-ORG\",  # Organization entities\n",
        "            \"B-LOC\", \"I-LOC\",  # Location entities\n",
        "            \"B-MISC\", \"I-MISC\"  # Miscellaneous entities\n",
        "        ]\n",
        "\n",
        "        # Create mappings between labels and IDs\n",
        "        self.label2id = {label: i for i, label in enumerate(self.label_list)}\n",
        "        self.id2label = {i: label for i, label in enumerate(self.label_list)}\n",
        "\n",
        "    def load_conll2003_dataset(self):\n",
        "        \"\"\"\n",
        "        Load the CoNLL-2003 dataset from HuggingFace datasets.\n",
        "\n",
        "        Returns:\n",
        "            dataset: The loaded dataset with train, validation, and test splits\n",
        "        \"\"\"\n",
        "        print(\"Loading CoNLL-2003 dataset...\")\n",
        "        dataset = load_dataset(\"conll2003\")\n",
        "        print(f\"Dataset loaded with {len(dataset['train'])} training, \"\n",
        "              f\"{len(dataset['validation'])} validation, and \"\n",
        "              f\"{len(dataset['test'])} test examples\")\n",
        "        return dataset\n",
        "\n",
        "    def convert_to_ner_format(self, dataset):\n",
        "        \"\"\"\n",
        "        Prepare dataset for NER training.\n",
        "\n",
        "        For CoNLL-2003, no conversion is needed as it's already\n",
        "        in the correct format for NER.\n",
        "        \"\"\"\n",
        "        # CoNLL-2003 is already in the correct format\n",
        "        return dataset"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XaIaMfXwN3p"
      },
      "source": [
        "## 3. Tokenizer\n",
        "\n",
        "Now, let's create the tokenizer class that handles subword tokenization and aligns entity labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "law75MpGwN3p"
      },
      "source": [
        "class PIITokenizer:\n",
        "    \"\"\"\n",
        "    Tokenizer that handles word to subword alignment for NER tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, label2id: Dict[str, int]):\n",
        "        \"\"\"\n",
        "        Initialize the PII tokenizer.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: A Hugging Face tokenizer (e.g., AutoTokenizer)\n",
        "            label2id: Mapping from label names to IDs\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "\n",
        "    def tokenize_and_align_labels(self, examples):\n",
        "        \"\"\"\n",
        "        Tokenize examples and align labels with subword tokens.\n",
        "\n",
        "        This function handles the conversion from word-level labels\n",
        "        to subword-level labels, using -100 as a special value for\n",
        "        ignored positions (subword continuations).\n",
        "        \"\"\"\n",
        "        # Tokenize the input words and get word IDs to align labels\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            examples[\"tokens\"],  # List of tokens for each example\n",
        "            truncation=True,  # Truncate to max length if needed\n",
        "            is_split_into_words=True,  # Input is already split into words\n",
        "            padding='max_length',  # Pad to max length\n",
        "            max_length=128,  # Maximum sequence length\n",
        "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        # Align labels with subword tokens\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "            # Get word IDs for current example\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "\n",
        "            # Track previous word to handle subword tokens\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "\n",
        "            # For each token in the sequence:\n",
        "            for word_idx in word_ids:\n",
        "                # Special tokens have word_idx = None\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)  # Ignore special tokens for loss\n",
        "\n",
        "                # If this is a new word (not a subword continuation)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label[word_idx])  # Use actual label\n",
        "\n",
        "                # If this is a subword continuation\n",
        "                else:\n",
        "                    label_ids.append(-100)  # Ignore subword continuations\n",
        "\n",
        "                # Update the previous word index\n",
        "                previous_word_idx = word_idx\n",
        "\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        # Add aligned labels to tokenized inputs\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho5ZOL4swN3q"
      },
      "source": [
        "## 4. Metrics\n",
        "\n",
        "Now, let's create the metrics class that will evaluate the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BwuDBuYNwN3r"
      },
      "source": [
        "import numpy as np\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "class PIIMetrics:\n",
        "    \"\"\"\n",
        "    Computes metrics for evaluating NER model performance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, label_list: List[str]):\n",
        "        \"\"\"\n",
        "        Initialize the metrics calculator with the list of possible labels.\n",
        "        \"\"\"\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def compute_metrics(self, eval_prediction):\n",
        "        \"\"\"\n",
        "        Compute evaluation metrics from model predictions.\n",
        "        \"\"\"\n",
        "        # Unpack predictions and labels\n",
        "        predictions, labels = eval_prediction\n",
        "\n",
        "        # Get predicted label IDs (argmax along the class dimension)\n",
        "        predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "        # Convert predictions to label names, ignoring special tokens (-100)\n",
        "        true_predictions = [\n",
        "            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        # Convert reference labels to label names, ignoring special tokens (-100)\n",
        "        true_labels = [\n",
        "            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "            for prediction, label in zip(predictions, labels)\n",
        "        ]\n",
        "\n",
        "        # Compute seqeval metrics\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "            \"f1\": f1_score(true_labels, true_predictions),\n",
        "            \"precision\": precision_score(true_labels, true_predictions),\n",
        "            \"recall\": recall_score(true_labels, true_predictions),\n",
        "        }"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ohkkOybwN3r"
      },
      "source": [
        "## 5. Trainer\n",
        "\n",
        "Now, let's create the trainer class that will train and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IPKDfpgUwN3r"
      },
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "\n",
        "class PIITrainer:\n",
        "    \"\"\"\n",
        "    Trainer for the NER model used for PII detection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, datasets, label_list: List[str],\n",
        "                 id2label: Dict, label2id: Dict, output_dir: str = \"./pii-model\"):\n",
        "        \"\"\"\n",
        "        Initialize the trainer with model and data.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        print(f\"Initializing tokenizer and model from {model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.pii_tokenizer = PIITokenizer(self.tokenizer, label2id)\n",
        "\n",
        "        # Tokenize the datasets\n",
        "        print(\"Tokenizing datasets...\")\n",
        "        self.tokenized_datasets = datasets.map(\n",
        "            self.pii_tokenizer.tokenize_and_align_labels,\n",
        "            batched=True,\n",
        "            remove_columns=datasets[\"train\"].column_names\n",
        "        )\n",
        "\n",
        "        # Initialize model with the correct number of labels\n",
        "        print(f\"Initializing model with {len(label_list)} labels...\")\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=len(label_list),\n",
        "            id2label=id2label,\n",
        "            label2id=label2id\n",
        "        )\n",
        "\n",
        "        # Set up metrics calculator\n",
        "        self.metrics = PIIMetrics(label_list)\n",
        "\n",
        "        # Configure training arguments\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device_str = \"CUDA\" if use_cuda else \"CPU\"\n",
        "        print(f\"Using {device_str} for training\")\n",
        "\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            fp16=use_cuda,  # Use FP16 if CUDA is available\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            report_to=\"none\",\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=100,\n",
        "        )\n",
        "\n",
        "        # Initialize the trainer\n",
        "        self.trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=self.tokenized_datasets[\"train\"],\n",
        "            eval_dataset=self.tokenized_datasets[\"validation\"],\n",
        "            data_collator=DataCollatorForTokenClassification(self.tokenizer),\n",
        "            compute_metrics=self.metrics.compute_metrics,\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        print(\"Starting model training...\")\n",
        "        return self.trainer.train()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate the model on the validation set.\"\"\"\n",
        "        print(\"Evaluating model...\")\n",
        "        return self.trainer.evaluate()\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the model, tokenizer, and configuration files.\"\"\"\n",
        "        print(f\"Saving model to {self.output_dir}...\")\n",
        "        self.trainer.save_model(self.output_dir)\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "        self.model.config.save_pretrained(self.output_dir)\n",
        "        print(\"Model saved successfully!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNOyT_43wN3s"
      },
      "source": [
        "## 6. Anonymizer\n",
        "\n",
        "Next, let's create the anonymizer class that will use the trained model to detect and anonymize entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "sU7HsY9lwN3s"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "class PIIAnonymizer:\n",
        "    \"\"\"\n",
        "    Anonymizes personally identifiable information in text.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, device: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the anonymizer with a trained model.\n",
        "        \"\"\"\n",
        "        # Set device (use CUDA if available and not explicitly set to CPU)\n",
        "        if device is None:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        print(f\"Loading model from: {model_path}\")\n",
        "        print(f\"Device set to use {self.device}\")\n",
        "\n",
        "        try:\n",
        "            # Try to load from local directory\n",
        "            if os.path.exists(model_path) and os.path.isdir(model_path):\n",
        "                # Check for required files\n",
        "                if not os.path.exists(os.path.join(model_path, \"config.json\")):\n",
        "                    print(f\"Warning: config.json not found in {model_path}\")\n",
        "                    print(\"Directory contents:\")\n",
        "                    print(os.listdir(model_path))\n",
        "\n",
        "                # Load model and tokenizer\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                self.model = AutoModelForTokenClassification.from_pretrained(model_path).to(self.device)\n",
        "\n",
        "                # Load label mapping (if available)\n",
        "                if os.path.exists(os.path.join(model_path, \"id2label.json\")):\n",
        "                    with open(os.path.join(model_path, \"id2label.json\"), \"r\") as f:\n",
        "                        self.id2label = json.load(f)\n",
        "                else:\n",
        "                    # Fall back to model's config\n",
        "                    self.id2label = self.model.config.id2label\n",
        "            else:\n",
        "                # If local path doesn't exist, try using it as a model ID from HuggingFace Hub\n",
        "                print(f\"Model directory {model_path} not found, attempting to load from HuggingFace Hub\")\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "                self.model = AutoModelForTokenClassification.from_pretrained(model_path).to(self.device)\n",
        "                self.id2label = self.model.config.id2label\n",
        "\n",
        "            # Create NER pipeline for easy inference\n",
        "            self.nlp = pipeline(\n",
        "                \"ner\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if self.device == \"cuda\" else -1,\n",
        "                aggregation_strategy=\"simple\"  # Merge subword tokens\n",
        "            )\n",
        "            print(f\"Successfully loaded model and created pipeline\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def anonymize_text(self, text: str, style: str = \"tag\") -> str:\n",
        "        \"\"\"\n",
        "        Anonymize PII in text by replacing entities with placeholders.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Detect entities\n",
        "        entities = self.nlp(text)\n",
        "\n",
        "        # Process in reverse order to avoid index shifting\n",
        "        anonymized_text = text\n",
        "        for entity in reversed(entities):\n",
        "            if entity['entity_group'] != 'O':\n",
        "                start, end = entity[\"start\"], entity[\"end\"]\n",
        "\n",
        "                # Extract the entity type (remove B-, I- prefixes)\n",
        "                tag = entity['entity_group'].split('-')[-1] if '-' in entity['entity_group'] else entity['entity_group']\n",
        "\n",
        "                # Apply anonymization based on style\n",
        "                if style == \"tag\":\n",
        "                    replacement = f\"[{tag}]\"\n",
        "                elif style == \"mask\":\n",
        "                    replacement = \"X\" * (end - start)\n",
        "                else:  # redact\n",
        "                    replacement = \"[REDACTED]\"\n",
        "\n",
        "                # Replace entity in text\n",
        "                anonymized_text = anonymized_text[:start] + replacement + anonymized_text[end:]\n",
        "\n",
        "        return anonymized_text\n",
        "\n",
        "    def detect_entities(self, text: str, threshold: float = 0.7) -> list:\n",
        "        \"\"\"\n",
        "        Detect entities in text with improved boundary detection.\n",
        "        \"\"\"\n",
        "        # Simple implementation that just uses the pipeline\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Get entities from pipeline\n",
        "        entities = self.nlp(text)\n",
        "\n",
        "        # Convert to more user-friendly format\n",
        "        result = []\n",
        "        for entity in entities:\n",
        "            if entity['score'] >= threshold:\n",
        "                # Extract entity type without B-/I- prefix\n",
        "                entity_type = entity['entity_group'].split('-')[-1] if '-' in entity['entity_group'] else entity['entity_group']\n",
        "\n",
        "                result.append({\n",
        "                    \"type\": entity_type,\n",
        "                    \"text\": entity['word'],\n",
        "                    \"confidence\": entity['score']\n",
        "                })\n",
        "\n",
        "        return result"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DD7H9LqwN3t"
      },
      "source": [
        "## 7. Utility Functions\n",
        "\n",
        "Let's create utility functions for model management and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VQMP19NRwN3t"
      },
      "source": [
        "import shutil\n",
        "\n",
        "def save_model_metadata(output_dir, processor):\n",
        "    \"\"\"Save additional metadata files needed for the model.\"\"\"\n",
        "    print(f\"Saving extra model files to {output_dir}\")\n",
        "\n",
        "    # Save label list\n",
        "    with open(os.path.join(output_dir, \"label_list.txt\"), \"w\") as f:\n",
        "        f.write(\"\\n\".join(processor.label_list))\n",
        "\n",
        "    # Save id2label mapping\n",
        "    with open(os.path.join(output_dir, \"id2label.json\"), \"w\") as f:\n",
        "        json.dump(processor.id2label, f)\n",
        "\n",
        "    # Save label2id mapping\n",
        "    with open(os.path.join(output_dir, \"label2id.json\"), \"w\") as f:\n",
        "        json.dump(processor.label2id, f)\n",
        "\n",
        "\n",
        "def check_model_exists(model_dir):\n",
        "    \"\"\"Check if a trained model exists and is valid.\"\"\"\n",
        "    # Check if directory exists\n",
        "    if not os.path.exists(model_dir):\n",
        "        return False, False\n",
        "\n",
        "    # Check if directory is not empty\n",
        "    if not os.listdir(model_dir):\n",
        "        return True, False\n",
        "\n",
        "    # Check for essential files\n",
        "    required_files = [\"config.json\", \"pytorch_model.bin\"]\n",
        "    for file in required_files:\n",
        "        if not os.path.exists(os.path.join(model_dir, file)):\n",
        "            return True, False\n",
        "\n",
        "    return True, True\n",
        "\n",
        "\n",
        "def try_fallback_model(test_sentences):\n",
        "    \"\"\"Try using a fallback model if the custom model fails.\"\"\"\n",
        "    print(\"\\nFallback: Loading a pre-trained NER model from HuggingFace instead\")\n",
        "    try:\n",
        "        fallback_anonymizer = PIIAnonymizer(\n",
        "            model_path=\"dslim/bert-base-NER\",\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        print(\"\\n===== FALLBACK ANONYMIZATION RESULTS =====\\n\")\n",
        "        for sentence in test_sentences:\n",
        "            anonymized_sentence = fallback_anonymizer.anonymize_text(sentence)\n",
        "            print(f\"Original:   {sentence}\")\n",
        "            print(f\"Anonymized: {anonymized_sentence}\")\n",
        "            print(\"-\" * 50)\n",
        "    except Exception as fallback_error:\n",
        "        print(f\"\\nFallback also failed: {str(fallback_error)}\")\n",
        "        print(\"Please check your installation of transformers and ensure you have internet connectivity.\")\n",
        "\n",
        "\n",
        "def get_test_sentences():\n",
        "    \"\"\"Get a list of test sentences for demonstration.\"\"\"\n",
        "    return [\n",
        "        \"EU rejects German call to boycott British lamb.\",\n",
        "        \"Peter Blackburn works at Microsoft in Seattle.\",\n",
        "        \"BRUSSELS 1996-08-22 - The meeting took place at the Grand Hotel.\",\n",
        "        \"John Smith and Sarah Johnson attended the conference in New York City last week.\",\n",
        "        \"The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep.\",\n",
        "        \"Apple Inc. announced its new headquarters in Cupertino, California.\"\n",
        "    ]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8FxKNDYwN3t"
      },
      "source": [
        "## 8. Main Training and Testing Function\n",
        "\n",
        "Now, let's create a function that trains the model and tests it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "VQ5zCtjCwN3t"
      },
      "source": [
        "def train_and_test_model(output_dir=\"./model\", force_train=False):\n",
        "    \"\"\"Train (if needed) and test a PII anonymization model.\"\"\"\n",
        "    # --- 1. Check if model already exists ---\n",
        "    exists, is_valid = check_model_exists(output_dir)\n",
        "\n",
        "    if exists and is_valid and not force_train:\n",
        "        print(f\"âœ“ Valid trained model already exists at {output_dir}\")\n",
        "        print(\"  Skipping training to save time.\")\n",
        "        print(\"  (Use force_train=True to retrain anyway)\")\n",
        "    else:\n",
        "        if exists and not is_valid:\n",
        "            print(f\"! Found model directory at {output_dir} but it's incomplete or invalid\")\n",
        "            print(\"  Removing directory and retraining...\")\n",
        "            shutil.rmtree(output_dir)\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "        elif force_train:\n",
        "            print(f\"! Forcing retraining as requested\")\n",
        "            shutil.rmtree(output_dir, ignore_errors=True)\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "        else:\n",
        "            print(f\"! No existing model found at {output_dir}\")\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # --- 2. Data Loading and Preparation ---\n",
        "        print(\"\\n=== Data Loading and Preparation ===\")\n",
        "        processor = PIIDataProcessor()\n",
        "        datasets = processor.load_conll2003_dataset()\n",
        "        datasets = processor.convert_to_ner_format(datasets)\n",
        "\n",
        "        # --- 3. Training ---\n",
        "        print(\"\\n=== Model Training ===\")\n",
        "        trainer = PIITrainer(\n",
        "            model_name=\"distilbert-base-uncased\",\n",
        "            datasets=datasets,\n",
        "            label_list=processor.label_list,\n",
        "            id2label=processor.id2label,\n",
        "            label2id=processor.label2id,\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # --- 4. Evaluation ---\n",
        "        print(\"\\n=== Model Evaluation ===\")\n",
        "        results = trainer.evaluate()\n",
        "        print(f\"Evaluation results: {results}\")\n",
        "\n",
        "        # --- 5. Save Model ---\n",
        "        print(\"\\n=== Saving Model ===\")\n",
        "        trainer.save_model()\n",
        "        save_model_metadata(output_dir, processor)\n",
        "        print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    # --- 6. Test Anonymization ---\n",
        "    print(\"\\n=== Testing Anonymization ===\")\n",
        "    test_sentences = get_test_sentences()\n",
        "\n",
        "    try:\n",
        "        print(\"Initializing anonymizer...\")\n",
        "        anonymizer = PIIAnonymizer(\n",
        "            model_path=output_dir,\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "        print(\"\\n===== ANONYMIZATION RESULTS =====\\n\")\n",
        "        for sentence in test_sentences:\n",
        "            anonymized_sentence = anonymizer.anonymize_text(sentence)\n",
        "            print(f\"Original:   {sentence}\")\n",
        "            print(f\"Anonymized: {anonymized_sentence}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        return anonymizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in anonymization: {str(e)}\")\n",
        "        try_fallback_model(test_sentences)\n",
        "        return None"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdayW8YwwN3u"
      },
      "source": [
        "## 9. Run Training and Testing\n",
        "\n",
        "Now, let's train the model (if needed) and test the anonymization functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a2e194d28a8545e1b41be7056586eff7",
            "4b2eeb2062e948beb366db390256a2ff",
            "a90d8b54e61e4f3bbe90c9c0badf751b",
            "224eed9c2ce748fe94f267cb619925bb",
            "ee342577e00b4839b3bfeb71c8397725",
            "c5043768c69645e8a48b2ba63c12af0f",
            "b61a2992bc7140c19cefaed897bdbd35",
            "7146e26cad8d4f9e925e13465b299773",
            "e4bbb9d5377440e9bfb0abc4d8e152cd",
            "67c3f9884f1d443dbaff7ede35d0ea04",
            "52386447b9e345b3ad8a6e9885e404cd"
          ]
        },
        "id": "T0S3zhEzwN3u",
        "outputId": "a95e8c83-884d-423e-deef-3cba20a1b3e0"
      },
      "source": [
        "# Train and test the model (will skip training if valid model exists)\n",
        "MODEL_DIR = \"/content/pii-model\"\n",
        "anonymizer = train_and_test_model(MODEL_DIR, force_train=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! Found model directory at /content/pii-model but it's incomplete or invalid\n",
            "  Removing directory and retraining...\n",
            "\n",
            "=== Data Loading and Preparation ===\n",
            "Loading CoNLL-2003 dataset...\n",
            "Dataset loaded with 14041 training, 3250 validation, and 3453 test examples\n",
            "\n",
            "=== Model Training ===\n",
            "Initializing tokenizer and model from distilbert-base-uncased...\n",
            "Tokenizing datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2e194d28a8545e1b41be7056586eff7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model with 9 labels...\n",
            "Using CUDA for training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2634/2634 03:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.066000</td>\n",
              "      <td>0.051944</td>\n",
              "      <td>0.985022</td>\n",
              "      <td>0.922394</td>\n",
              "      <td>0.918238</td>\n",
              "      <td>0.926587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.041100</td>\n",
              "      <td>0.048593</td>\n",
              "      <td>0.987048</td>\n",
              "      <td>0.935743</td>\n",
              "      <td>0.929985</td>\n",
              "      <td>0.941573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.049156</td>\n",
              "      <td>0.987437</td>\n",
              "      <td>0.937903</td>\n",
              "      <td>0.933600</td>\n",
              "      <td>0.942246</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Evaluation ===\n",
            "Evaluating model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [204/204 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.04915635287761688, 'eval_accuracy': 0.9874374306137156, 'eval_f1': 0.9379032933880835, 'eval_precision': 0.9336002669336003, 'eval_recall': 0.942246169388786, 'eval_runtime': 4.5347, 'eval_samples_per_second': 716.702, 'eval_steps_per_second': 44.987, 'epoch': 3.0}\n",
            "\n",
            "=== Saving Model ===\n",
            "Saving model to /content/pii-model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "Saving extra model files to /content/pii-model\n",
            "Model saved to /content/pii-model\n",
            "\n",
            "=== Testing Anonymization ===\n",
            "Initializing anonymizer...\n",
            "Loading model from: /content/pii-model\n",
            "Device set to use cuda\n",
            "Successfully loaded model and created pipeline\n",
            "\n",
            "===== ANONYMIZATION RESULTS =====\n",
            "\n",
            "Original:   EU rejects German call to boycott British lamb.\n",
            "Anonymized: [ORG] rejects [MISC] call to boycott [MISC] lamb.\n",
            "--------------------------------------------------\n",
            "Original:   Peter Blackburn works at Microsoft in Seattle.\n",
            "Anonymized: [PER] works at [ORG] in [LOC].\n",
            "--------------------------------------------------\n",
            "Original:   BRUSSELS 1996-08-22 - The meeting took place at the Grand Hotel.\n",
            "Anonymized: [LOC] 1996-08-22 - The meeting took place at the [LOC].\n",
            "--------------------------------------------------\n",
            "Original:   John Smith and Sarah Johnson attended the conference in New York City last week.\n",
            "Anonymized: [PER] and [PER] attended the conference in [LOC] last week.\n",
            "--------------------------------------------------\n",
            "Original:   The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep.\n",
            "Anonymized: The [ORG] said on Thursday it disagreed with [MISC] advice to consumers to shun [MISC] lamb until scientists determine whether mad cow disease can be transmitted to sheep.\n",
            "--------------------------------------------------\n",
            "Original:   Apple Inc. announced its new headquarters in Cupertino, California.\n",
            "Anonymized: [ORG]. announced its new headquarters in [LOC][LOC], [LOC].\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHdV-hodwN3v"
      },
      "source": [
        "## 10. Try Different Anonymization Styles\n",
        "\n",
        "Let's try the different anonymization styles available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lUJpNQDswN3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ba2869-9070-47e4-a524-b0226b607acb"
      },
      "source": [
        "# Function to demonstrate different anonymization styles\n",
        "def demo_anonymization_styles(text):\n",
        "    if anonymizer is None:\n",
        "        print(\"Anonymizer not available. Please check previous steps.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Original text: {text}\\n\")\n",
        "\n",
        "    print(\"Tag style (default):\")\n",
        "    print(anonymizer.anonymize_text(text, style=\"tag\"))\n",
        "    print(\"\\nMask style:\")\n",
        "    print(anonymizer.anonymize_text(text, style=\"mask\"))\n",
        "    print(\"\\nRedact style:\")\n",
        "    print(anonymizer.anonymize_text(text, style=\"redact\"))\n",
        "\n",
        "    print(\"\\nDetected entities:\")\n",
        "    entities = anonymizer.detect_entities(text, threshold=0.6)\n",
        "    for entity in entities:\n",
        "        print(f\"â€¢ {entity['text']} - {entity['type']} ({entity['confidence']:.2%})\")\n",
        "\n",
        "# Try with a custom example\n",
        "demo_text = \"John Smith is the CEO of Acme Corporation based in New York City.\"\n",
        "demo_anonymization_styles(demo_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: John Smith is the CEO of Acme Corporation based in New York City.\n",
            "\n",
            "Tag style (default):\n",
            "[PER] is the CEO of [ORG] based in [LOC].\n",
            "\n",
            "Mask style:\n",
            "XXXXXXXXXX is the CEO of XXXXXXXXXXXXXXXX based in XXXXXXXXXXXXX.\n",
            "\n",
            "Redact style:\n",
            "[REDACTED] is the CEO of [REDACTED] based in [REDACTED].\n",
            "\n",
            "Detected entities:\n",
            "â€¢ john smith - PER (99.77%)\n",
            "â€¢ acme corporation - ORG (99.41%)\n",
            "â€¢ new york city - LOC (99.46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7x5Tuo_wN3v"
      },
      "source": [
        "## 11. Interactive Demo with Gradio\n",
        "\n",
        "Now, let's create an interactive web interface using Gradio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "WD-LpQT1wN3v"
      },
      "source": [
        "# Install gradio if not already installed\n",
        "!pip install -q gradio"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zpJ0cTOQwN3v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "18ba42c7-3e60-4c7f-9fa5-2b45ad19b21e"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def create_gradio_interface(anon):\n",
        "    \"\"\"\n",
        "    Create a Gradio interface for PII anonymization.\n",
        "    \"\"\"\n",
        "    if anon is None:\n",
        "        print(\"Anonymizer not available. Can't create interface.\")\n",
        "        return None\n",
        "\n",
        "    def process_text(text, threshold, style, show_entities):\n",
        "        \"\"\"Process text for the Gradio interface.\"\"\"\n",
        "        if not text:\n",
        "            return \"Please enter some text to anonymize.\"\n",
        "\n",
        "        try:\n",
        "            # Detect entities\n",
        "            entities = anon.detect_entities(text, threshold)\n",
        "\n",
        "            # Anonymize text\n",
        "            anonymized = anon.anonymize_text(text, style)\n",
        "\n",
        "            # Return result with or without entity details\n",
        "            if show_entities:\n",
        "                entity_list = \"\\n\".join([\n",
        "                    f\"â€¢ {e['text']} - {e['type']} ({e['confidence']:.2%})\"\n",
        "                    for e in entities\n",
        "                ])\n",
        "                return f\"Anonymized text:\\n{anonymized}\\n\\nDetected entities:\\n{entity_list}\"\n",
        "            else:\n",
        "                return anonymized\n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            return f\"Error processing text: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
        "\n",
        "    def highlight_entities(text, threshold=0.7):\n",
        "        \"\"\"Generate HTML with highlighted entities for visualization.\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Detect entities\n",
        "        entities = anon.detect_entities(text, threshold)\n",
        "\n",
        "        # Define entity type colors\n",
        "        colors = {\n",
        "            \"PER\": \"#ffcccc\",  # Light red for persons\n",
        "            \"ORG\": \"#ccffcc\",  # Light green for organizations\n",
        "            \"LOC\": \"#ccccff\",  # Light blue for locations\n",
        "            \"MISC\": \"#ffffcc\"  # Light yellow for miscellaneous\n",
        "        }\n",
        "\n",
        "        # Find entity positions in text\n",
        "        entities_with_pos = []\n",
        "        for entity in entities:\n",
        "            start = text.find(entity[\"text\"])\n",
        "            if start >= 0:\n",
        "                entities_with_pos.append({\n",
        "                    \"start\": start,\n",
        "                    \"end\": start + len(entity[\"text\"]),\n",
        "                    \"type\": entity[\"type\"],\n",
        "                    \"confidence\": entity[\"confidence\"]\n",
        "                })\n",
        "\n",
        "        # Sort by position reversed (to avoid index shifting)\n",
        "        entities_with_pos.sort(key=lambda x: x[\"start\"], reverse=True)\n",
        "\n",
        "        # Insert HTML tags for highlighting\n",
        "        html_text = text\n",
        "        for entity in entities_with_pos:\n",
        "            entity_text = text[entity[\"start\"]:entity[\"end\"]]\n",
        "            entity_type = entity[\"type\"]\n",
        "            confidence = entity[\"confidence\"]\n",
        "            color = colors.get(entity_type, \"#eeeeee\")\n",
        "\n",
        "            # Create highlighted span\n",
        "            html_entity = (\n",
        "                f'<span style=\"background-color: {color};\" '\n",
        "                f'title=\"{entity_type} ({confidence:.1%})\">{entity_text}</span>'\n",
        "            )\n",
        "\n",
        "            # Replace text with highlighted version\n",
        "            html_text = html_text[:entity[\"start\"]] + html_entity + html_text[entity[\"end\"]:]\n",
        "\n",
        "        return html_text\n",
        "\n",
        "    # Create interface with tabs for different functionalities\n",
        "    with gr.Blocks(title=\"PII Anonymization Tool\") as demo:\n",
        "        gr.Markdown(\"# PII Anonymization Tool\")\n",
        "        gr.Markdown(\"\"\"\n",
        "        This tool automatically detects and anonymizes personally identifiable information in text\n",
        "        using a fine-tuned Named Entity Recognition model.\n",
        "\n",
        "        It can detect names, organizations, locations, and other entities in your text.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"Text Anonymization\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    # Input area\n",
        "                    input_text = gr.Textbox(\n",
        "                        lines=5,\n",
        "                        placeholder=\"Enter text to anonymize (e.g., 'John Smith works at Microsoft in New York.')\",\n",
        "                        label=\"Input Text\"\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        # Configuration options\n",
        "                        threshold = gr.Slider(\n",
        "                            minimum=0.1,\n",
        "                            maximum=0.9,\n",
        "                            value=0.6,\n",
        "                            step=0.05,\n",
        "                            label=\"Confidence Threshold (higher = fewer replacements)\"\n",
        "                        )\n",
        "                        style = gr.Radio(\n",
        "                            [\"tag\", \"mask\", \"redact\"],\n",
        "                            label=\"Anonymization Style\",\n",
        "                            value=\"tag\",\n",
        "                            info=\"Tag: [PER], Mask: XXXX, Redact: [REDACTED]\"\n",
        "                        )\n",
        "\n",
        "                    show_entities = gr.Checkbox(\n",
        "                        label=\"Show detected entities\",\n",
        "                        value=True\n",
        "                    )\n",
        "\n",
        "                    anonymize_btn = gr.Button(\"Anonymize Text\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    # Output area\n",
        "                    output_text = gr.Textbox(label=\"Result\", lines=10)\n",
        "\n",
        "            # Example inputs\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"John Smith works at Microsoft in New York.\", 0.6, \"tag\", True],\n",
        "                    [\"Please contact Sarah Johnson at sarah.j@example.com or call 555-123-4567.\", 0.5, \"tag\", True],\n",
        "                    [\"Patient #12345 was admitted on January 15th with Dr. Williams supervising.\", 0.7, \"mask\", False],\n",
        "                    [\"EU rejects German call to boycott British lamb.\", 0.6, \"tag\", True],\n",
        "                    [\"Apple Inc. announced its new headquarters in Cupertino, California.\", 0.7, \"tag\", True]\n",
        "                ],\n",
        "                inputs=[input_text, threshold, style, show_entities]\n",
        "            )\n",
        "\n",
        "            # Connect button to processing function\n",
        "            anonymize_btn.click(\n",
        "                fn=process_text,\n",
        "                inputs=[input_text, threshold, style, show_entities],\n",
        "                outputs=output_text\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Entity Highlighting\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    # Input area\n",
        "                    highlight_input = gr.Textbox(\n",
        "                        lines=5,\n",
        "                        placeholder=\"Enter text to highlight entities\",\n",
        "                        label=\"Input Text\"\n",
        "                    )\n",
        "                    highlight_threshold = gr.Slider(\n",
        "                        minimum=0.1,\n",
        "                        maximum=0.9,\n",
        "                        value=0.6,\n",
        "                        step=0.05,\n",
        "                        label=\"Confidence Threshold\"\n",
        "                    )\n",
        "                    highlight_btn = gr.Button(\"Highlight Entities\")\n",
        "                with gr.Column():\n",
        "                    # Output HTML with highlighting\n",
        "                    highlighted_output = gr.HTML(label=\"Highlighted Text\")\n",
        "\n",
        "            # Connect button to highlighting function\n",
        "            highlight_btn.click(\n",
        "                fn=highlight_entities,\n",
        "                inputs=[highlight_input, highlight_threshold],\n",
        "                outputs=highlighted_output\n",
        "            )\n",
        "\n",
        "            # Example inputs for highlighting\n",
        "            highlight_examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"John Smith works at Microsoft in New York.\", 0.6],\n",
        "                    [\"EU rejects German call to boycott British lamb.\", 0.6],\n",
        "                    [\"The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb.\", 0.7]\n",
        "                ],\n",
        "                inputs=[highlight_input, highlight_threshold]\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"About\"):\n",
        "            # About tab with information about the project\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## About This Tool\n",
        "\n",
        "            This PII anonymization tool uses a fine-tuned Named Entity Recognition (NER) model\n",
        "            based on DistilBERT to identify and anonymize personally identifiable information in text.\n",
        "\n",
        "            ### Entity Types\n",
        "            - **PER**: Person names (e.g., \"John Smith\")\n",
        "            - **ORG**: Organizations (e.g., \"Microsoft\", \"European Commission\")\n",
        "            - **LOC**: Locations (e.g., \"New York\", \"Brussels\")\n",
        "            - **MISC**: Miscellaneous entities (e.g., \"German\", \"British\")\n",
        "\n",
        "            ### Anonymization Styles\n",
        "            - **Tag**: Replaces entities with their type in brackets, e.g., [PER]\n",
        "            - **Mask**: Replaces characters with 'X', maintaining the entity length\n",
        "            - **Redact**: Replaces entities with [REDACTED], regardless of entity type or length\n",
        "\n",
        "            ### Model Information\n",
        "            This tool uses a model fine-tuned on CoNLL-2003 dataset for named entity recognition.\n",
        "            \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Create and launch the interface\n",
        "if anonymizer is not None:\n",
        "    demo = create_gradio_interface(anonymizer)\n",
        "    if demo is not None:\n",
        "        demo.launch(share=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ae23bf084ba851acc0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae23bf084ba851acc0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:"
      ],
      "metadata": {
        "id": "VGvhIow8fiYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n",
        "    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n",
        "    author = \"Tjong Kim Sang, Erik F.  and De Meulder, Fien\",\n",
        "    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n",
        "    year = \"2003\",\n",
        "    url = \"https://www.aclweb.org/anthology/W03-0419\",\n",
        "    pages = \"142--147\",\n",
        "}"
      ],
      "metadata": {
        "id": "bQOrX3CafgmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n",
        "    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n",
        "    author = \"Tjong Kim Sang, Erik F.  and De Meulder, Fien\",\n",
        "    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n",
        "    year = \"2003\",\n",
        "    url = \"https://www.aclweb.org/anthology/W03-0419\",\n",
        "    pages = \"142--147\",\n",
        "}"
      ],
      "metadata": {
        "id": "vd_A3G2ofdYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2e194d28a8545e1b41be7056586eff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b2eeb2062e948beb366db390256a2ff",
              "IPY_MODEL_a90d8b54e61e4f3bbe90c9c0badf751b",
              "IPY_MODEL_224eed9c2ce748fe94f267cb619925bb"
            ],
            "layout": "IPY_MODEL_ee342577e00b4839b3bfeb71c8397725"
          }
        },
        "4b2eeb2062e948beb366db390256a2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5043768c69645e8a48b2ba63c12af0f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b61a2992bc7140c19cefaed897bdbd35",
            "value": "Map:â€‡100%"
          }
        },
        "a90d8b54e61e4f3bbe90c9c0badf751b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7146e26cad8d4f9e925e13465b299773",
            "max": 3250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4bbb9d5377440e9bfb0abc4d8e152cd",
            "value": 3250
          }
        },
        "224eed9c2ce748fe94f267cb619925bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67c3f9884f1d443dbaff7ede35d0ea04",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_52386447b9e345b3ad8a6e9885e404cd",
            "value": "â€‡3250/3250â€‡[00:00&lt;00:00,â€‡4022.84â€‡examples/s]"
          }
        },
        "ee342577e00b4839b3bfeb71c8397725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5043768c69645e8a48b2ba63c12af0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b61a2992bc7140c19cefaed897bdbd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7146e26cad8d4f9e925e13465b299773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4bbb9d5377440e9bfb0abc4d8e152cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67c3f9884f1d443dbaff7ede35d0ea04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52386447b9e345b3ad8a6e9885e404cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}